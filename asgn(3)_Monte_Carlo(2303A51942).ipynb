{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPH2kRcpXQMtZgiAR/JDRYP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RAVITEJA-VADLURI/Mishra-sir-s-assignments/blob/main/asgn(3)_Monte_Carlo(2303A51942).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXVr0OLYVsQM",
        "outputId": "0d4f00c9-70b6-4edf-dc2e-8fa39085c69c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Monte Carlo Policy Evaluation (V):\n",
            "[[-0.43  0.63  1.81  3.12  4.58]\n",
            " [ 0.    0.    0.    0.    6.2 ]\n",
            " [ 0.    0.    0.    0.    8.  ]\n",
            " [ 0.    0.    0.    0.   10.  ]\n",
            " [ 0.    0.    0.    0.    0.  ]]\n",
            "\n",
            "Monte Carlo Control Policy:\n",
            "[[2 2 0 0 2]\n",
            " [0 0 0 0 2]\n",
            " [2 2 2 2 2]\n",
            " [2 2 0 2 2]\n",
            " [0 0 0 0 1]]\n"
          ]
        }
      ],
      "source": [
        "#Implementing Monte Carlo Methods for Policy Evaluation and Control\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "N = 5\n",
        "actions = [(0,1),(0,-1),(1,0),(-1,0)]  # Right, Left, Down, Up\n",
        "action_labels = [\"R\",\"L\",\"D\",\"U\"]\n",
        "goal, pit = (4,4), (2,2)\n",
        "gamma = 0.9\n",
        "\n",
        "def reward(state):\n",
        "    if state == goal: return 10\n",
        "    if state == pit: return -10\n",
        "    return -1\n",
        "\n",
        "def next_state(state, action):\n",
        "    if state in [goal,pit]:\n",
        "        return state\n",
        "    x,y = state\n",
        "    dx,dy = action\n",
        "    nx,ny = x+dx, y+dy\n",
        "    if 0 <= nx < N and 0 <= ny < N:\n",
        "        return (nx,ny)\n",
        "    return state  # invalid -> stay\n",
        "\n",
        "# Generate an episode following policy\n",
        "def generate_episode(policy, max_steps=50):\n",
        "    episode = []\n",
        "    state = (0,0)  # start\n",
        "    for _ in range(max_steps):\n",
        "        if state in [goal,pit]:\n",
        "            break\n",
        "        action_idx = policy[state]\n",
        "        action = actions[action_idx]\n",
        "        next_s = next_state(state, action)\n",
        "        r = reward(next_s)\n",
        "        episode.append((state, action_idx, r))\n",
        "        state = next_s\n",
        "    return episode\n",
        "\n",
        "# Monte Carlo Policy Evaluation\n",
        "def mc_policy_evaluation(policy, episodes=5000):\n",
        "    returns_sum = {}\n",
        "    returns_count = {}\n",
        "    V = np.zeros((N,N))\n",
        "\n",
        "    for _ in range(episodes):\n",
        "        ep = generate_episode(policy)\n",
        "        G = 0\n",
        "        visited = set()\n",
        "        for t in reversed(range(len(ep))):\n",
        "            s,a,r = ep[t]\n",
        "            G = gamma*G + r\n",
        "            if s not in visited:\n",
        "                visited.add(s)\n",
        "                returns_sum[s] = returns_sum.get(s,0)+G\n",
        "                returns_count[s] = returns_count.get(s,0)+1\n",
        "                V[s] = returns_sum[s]/returns_count[s]\n",
        "    return V\n",
        "\n",
        "# Monte Carlo Control with epsilon-greedy\n",
        "def mc_control_epsilon_greedy(episodes=10000, epsilon=0.1):\n",
        "    Q = np.zeros((N,N,len(actions)))\n",
        "    returns_sum, returns_count = {}, {}\n",
        "    policy = np.random.choice(len(actions), size=(N,N))\n",
        "\n",
        "    for _ in range(episodes):\n",
        "        # Îµ-greedy action selection\n",
        "        def choose_action(s):\n",
        "            if random.random() < epsilon:\n",
        "                return np.random.choice(len(actions))\n",
        "            else:\n",
        "                return np.argmax(Q[s])\n",
        "\n",
        "        # generate episode\n",
        "        episode=[]\n",
        "        state=(0,0)\n",
        "        for t in range(50):\n",
        "            if state in [goal,pit]:\n",
        "                break\n",
        "            a=choose_action(state)\n",
        "            next_s=next_state(state,actions[a])\n",
        "            r=reward(next_s)\n",
        "            episode.append((state,a,r))\n",
        "            state=next_s\n",
        "\n",
        "        # update returns\n",
        "        G=0\n",
        "        visited=set()\n",
        "        for t in reversed(range(len(episode))):\n",
        "            s,a,r=episode[t]\n",
        "            G=gamma*G+r\n",
        "            if (s,a) not in visited:\n",
        "                visited.add((s,a))\n",
        "                returns_sum[(s,a)]=returns_sum.get((s,a),0)+G\n",
        "                returns_count[(s,a)]=returns_count.get((s,a),0)+1\n",
        "                Q[s][a]=returns_sum[(s,a)]/returns_count[(s,a)]\n",
        "                policy[s]=np.argmax(Q[s])  # greedy improvement\n",
        "\n",
        "    return Q, policy\n",
        "\n",
        "# ---------------- Run ----------------\n",
        "# Fixed policy: always Right if possible, else Down\n",
        "fixed_policy = np.zeros((N,N),dtype=int)  # all \"Right\" by default\n",
        "for i in range(N):\n",
        "    for j in range(N):\n",
        "        if j==N-1: fixed_policy[i,j]=2  # Down if at right border\n",
        "\n",
        "V_mc = mc_policy_evaluation(fixed_policy, episodes=5000)\n",
        "print(\"Monte Carlo Policy Evaluation (V):\")\n",
        "print(np.round(V_mc,2))\n",
        "\n",
        "Q_mc, pi_mc = mc_control_epsilon_greedy(episodes=10000)\n",
        "print(\"\\nMonte Carlo Control Policy:\")\n",
        "print(pi_mc)\n"
      ]
    }
  ]
}